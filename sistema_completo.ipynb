{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7983feaf",
   "metadata": {},
   "source": [
    "# Sistema completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cd35f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "LEFT_EYE_FULL = [33, 160, 158, 133, 153, 144]\n",
    "RIGHT_EYE_FULL = [362, 385, 387, 263, 373, 380]\n",
    "\n",
    "def eye_aspect_ratio(landmarks, eye_indices, image_shape):\n",
    "    def dist(p1, p2):\n",
    "        x1, y1 = int(p1.x * image_shape[1]), int(p1.y * image_shape[0])\n",
    "        x2, y2 = int(p2.x * image_shape[1]), int(p2.y * image_shape[0])\n",
    "        return ((x2 - x1)**2 + (y2 - y1)**2) ** 0.5\n",
    "    A = dist(landmarks[eye_indices[1]], landmarks[eye_indices[5]])\n",
    "    B = dist(landmarks[eye_indices[2]], landmarks[eye_indices[4]])\n",
    "    D = dist(landmarks[eye_indices[0]], landmarks[eye_indices[3]])\n",
    "    return (A + B) / (2.0 * D)\n",
    "\n",
    "def is_looking_forward(landmarks):\n",
    "    def centered(pupil, outer, inner):\n",
    "        d1 = abs(pupil - outer)\n",
    "        d2 = abs(inner - pupil)\n",
    "        ratio = d1 / (d1 + d2 + 1e-6)\n",
    "        return 0.425 < ratio < 0.575\n",
    "    left = centered(landmarks[468].x, landmarks[33].x, landmarks[133].x)\n",
    "    right = centered(landmarks[473].x, landmarks[362].x, landmarks[263].x)\n",
    "    return left and right\n",
    "\n",
    "def realizar_reto_de_vida():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    frame_width, frame_height = int(cap.get(3)), int(cap.get(4))\n",
    "    out = cv2.VideoWriter(\"verificacion_video.mp4\", cv2.VideoWriter_fourcc(*'mp4v'), 20, (frame_width, frame_height))\n",
    "    face_mesh = mp_face_mesh.FaceMesh(refine_landmarks=True)\n",
    "\n",
    "    # Estado\n",
    "    blink_counter, blink_ready = 0, True\n",
    "    blink_start_time, blink_completed = None, False\n",
    "    nod_count, nod_stage, nod_start_y, nod_start_time, nod_completed = 0, \"neutral\", None, None, False\n",
    "    shake_count, shake_stage, shake_start_x, shake_start_time, shake_completed = 0, \"neutral\", None, None, False\n",
    "    stable_frame, stable_counter = None, 0\n",
    "    gaze_start_time, selfie_taken = None, False\n",
    "    fase_selfie = False\n",
    "\n",
    "    print(\"üü° Iniciando reto de vida...\")\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        out.write(frame)\n",
    "        raw_frame = frame.copy()\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(rgb)\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            face_landmarks = results.multi_face_landmarks[0]\n",
    "            mp_drawing.draw_landmarks(frame, face_landmarks, mp_face_mesh.FACEMESH_TESSELATION)\n",
    "\n",
    "            ear_left = eye_aspect_ratio(face_landmarks.landmark, LEFT_EYE_FULL, frame.shape)\n",
    "            ear_right = eye_aspect_ratio(face_landmarks.landmark, RIGHT_EYE_FULL, frame.shape)\n",
    "            ear_avg = (ear_left + ear_right) / 2.0\n",
    "\n",
    "            if not fase_selfie:\n",
    "                # Parpadeo\n",
    "                if not blink_completed:\n",
    "                    if ear_avg < 0.26 and blink_ready:\n",
    "                        if blink_counter == 0:\n",
    "                            blink_start_time = time.time()\n",
    "                        blink_counter += 1\n",
    "                        blink_ready = False\n",
    "                        print(f\"‚úÖ Parpadeo #{blink_counter}\")\n",
    "                    elif ear_avg >= 0.30:\n",
    "                        blink_ready = True\n",
    "                    if blink_counter > 0 and (time.time() - blink_start_time > 3):\n",
    "                        print(\"‚è±Ô∏è Parpadeos: tiempo excedido\")\n",
    "                        blink_counter, blink_start_time = 0, None\n",
    "                    if blink_counter >= 3:\n",
    "                        blink_completed = True\n",
    "                        print(\"‚úÖ Parpadeos completados\")\n",
    "\n",
    "                # Asentir (\"s√≠\")\n",
    "                nose_y = face_landmarks.landmark[1].y\n",
    "                if nod_start_y is None:\n",
    "                    nod_start_y = nose_y\n",
    "                    nod_start_time = time.time()\n",
    "                delta_y = nose_y - nod_start_y\n",
    "                if not nod_completed:\n",
    "                    if nod_stage == \"neutral\" and delta_y > 0.03:\n",
    "                        nod_stage = \"down\"\n",
    "                    elif nod_stage == \"down\" and delta_y < -0.03:\n",
    "                        nod_stage = \"up\"\n",
    "                        nod_count += 1\n",
    "                        print(f\"‚úÖ Asentimiento #{nod_count}\")\n",
    "                        nod_stage = \"neutral\"\n",
    "                    if time.time() - nod_start_time > 5 and nod_count < 2:\n",
    "                        print(\"‚è±Ô∏è Asentir: tiempo excedido\")\n",
    "                        nod_count, nod_start_time = 0, time.time()\n",
    "                    if nod_count >= 2:\n",
    "                        nod_completed = True\n",
    "                        print(\"‚úÖ Asentir completado\")\n",
    "\n",
    "                # Negar (\"no\")\n",
    "                nose_x = face_landmarks.landmark[1].x\n",
    "                if shake_start_x is None:\n",
    "                    shake_start_x = nose_x\n",
    "                    shake_start_time = time.time()\n",
    "                delta_x = nose_x - shake_start_x\n",
    "                if not shake_completed:\n",
    "                    if shake_stage == \"neutral\" and delta_x > 0.03:\n",
    "                        shake_stage = \"right\"\n",
    "                    elif shake_stage == \"right\" and delta_x < -0.03:\n",
    "                        shake_stage = \"left\"\n",
    "                        shake_count += 1\n",
    "                        print(f\"‚úÖ Negaci√≥n #{shake_count}\")\n",
    "                        shake_stage = \"neutral\"\n",
    "                    if time.time() - shake_start_time > 5 and shake_count < 2:\n",
    "                        print(\"‚è±Ô∏è Negar: tiempo excedido\")\n",
    "                        shake_count, shake_start_time = 0, time.time()\n",
    "                    if shake_count >= 2:\n",
    "                        shake_completed = True\n",
    "                        print(\"‚úÖ Negar completado\")\n",
    "\n",
    "                # Mostrar progreso\n",
    "                cv2.putText(frame, f\"Parpadeos: {blink_counter}/3\", (30, 40),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "                cv2.putText(frame, f\"Asentir: {nod_count}/2\", (30, 70),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)\n",
    "                cv2.putText(frame, f\"Negar: {shake_count}/2\", (30, 100),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 255), 2)\n",
    "\n",
    "                if blink_completed and nod_completed and shake_completed:\n",
    "                    print(\"üì∏ Reto de vida completado. Prepara tu selfie...\")\n",
    "                    fase_selfie = True\n",
    "                    start_selfie_time = time.time()\n",
    "                    continue\n",
    "\n",
    "            if fase_selfie and not selfie_taken:\n",
    "                nose_x = face_landmarks.landmark[1].x\n",
    "                nose_y = face_landmarks.landmark[1].y\n",
    "                centered = 0.4 < nose_x < 0.6 and 0.4 < nose_y < 0.6\n",
    "                eyes_open = ear_left > 0.26 and ear_right > 0.26\n",
    "\n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                gray = cv2.GaussianBlur(gray, (21, 21), 0)\n",
    "                if stable_frame is None:\n",
    "                    stable_frame = gray\n",
    "                    continue\n",
    "                diff = cv2.absdiff(stable_frame, gray)\n",
    "                _, thresh = cv2.threshold(diff, 25, 255, cv2.THRESH_BINARY)\n",
    "                movement = cv2.countNonZero(thresh)\n",
    "                stable = movement < 5000\n",
    "                stable_counter = stable_counter + 1 if stable else 0\n",
    "\n",
    "                looking_forward = is_looking_forward(face_landmarks.landmark)\n",
    "                if looking_forward:\n",
    "                    if gaze_start_time is None:\n",
    "                        gaze_start_time = time.time()\n",
    "                    gaze_duration = time.time() - gaze_start_time\n",
    "                else:\n",
    "                    gaze_start_time = None\n",
    "                    gaze_duration = 0\n",
    "\n",
    "                cv2.putText(frame, f\"Cara centrada: {'‚úÖ' if centered else '‚ùå'}\", (30, 140),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (200, 255, 200), 2)\n",
    "                cv2.putText(frame, f\"Ojos abiertos: {'‚úÖ' if eyes_open else '‚ùå'}\", (30, 170),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (200, 255, 200), 2)\n",
    "                cv2.putText(frame, f\"Estable: {'‚úÖ' if stable_counter >= 5 else '‚ùå'}\", (30, 200),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (200, 255, 200), 2)\n",
    "                cv2.putText(frame, f\"Mirando: {'‚úÖ' if gaze_duration >= 3 else f'{gaze_duration:.1f}s'}\", (30, 230),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (200, 255, 200), 2)\n",
    "\n",
    "                if centered and eyes_open and stable_counter >= 5 and gaze_duration >= 3:\n",
    "                    cv2.imwrite(\"selfie.jpg\", raw_frame)\n",
    "                    print(\"üì∑ Selfie capturada como 'selfie.jpg'\")\n",
    "                    selfie_taken = True\n",
    "                    break\n",
    "                stable_frame = gray\n",
    "\n",
    "        cv2.imshow(\"Verificaci√≥n Facial\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"üé¨ Reto de vida completado correctamente.\")\n",
    "    if selfie_taken == True:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b107b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬øGPU disponible?: True\n",
      "GPU actual: NVIDIA GeForce RTX 4070 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"¬øGPU disponible?:\", torch.cuda.is_available())\n",
    "print(\"GPU actual:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"Ninguna\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e11a5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verificar_ine_con_curp():\n",
    "    import cv2\n",
    "    import time\n",
    "    import re\n",
    "    import imutils\n",
    "    from selenium import webdriver\n",
    "    from selenium.common.exceptions import WebDriverException, TimeoutException\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from paddleocr import PaddleOCR\n",
    "\n",
    "    def coincide_nombre_web_con_ocr(nombre_web, ap1, ap2, lineas_ocr):\n",
    "        partes = [nombre_web, ap1, ap2]\n",
    "        for parte in partes:\n",
    "            if not any(parte.upper() in linea for linea in lineas_ocr):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    ocr = PaddleOCR(use_angle_cls=True, lang='es')\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "    while True:\n",
    "        print(\"ü™™ Muestra tu INE al centro de la c√°mara, bien enfocada y estable...\")\n",
    "\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        stable_frame = None\n",
    "        ine_capturada = False\n",
    "        stable_counter = 0\n",
    "        required_frames = 150\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, raw_frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame = raw_frame.copy()\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "            edged = cv2.Canny(blur, 75, 200)\n",
    "            cnts = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cnts = imutils.grab_contours(cnts)\n",
    "            cnts = sorted(cnts, key=cv2.contourArea, reverse=True)[:5]\n",
    "\n",
    "            found_card = False\n",
    "            frame_area = frame.shape[0] * frame.shape[1]\n",
    "\n",
    "            for c in cnts:\n",
    "                approx = cv2.approxPolyDP(c, 0.02 * cv2.arcLength(c, True), True)\n",
    "                area = cv2.contourArea(c)\n",
    "                if len(approx) == 4 and area > 0.20 * frame_area:\n",
    "                    cv2.drawContours(frame, [approx], -1, (0, 255, 0), 2)\n",
    "                    stable_counter += 1\n",
    "                    found_card = True\n",
    "                    break\n",
    "\n",
    "            if not found_card:\n",
    "                stable_counter = 0\n",
    "\n",
    "            porcentaje = int((stable_counter / required_frames) * 100)\n",
    "            cv2.putText(frame, f\"INE detectada: {porcentaje}%\", (20, 40),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n",
    "            cv2.putText(frame, \"Mant√©n la INE visible y grande durante 5 segundos\",\n",
    "                        (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "\n",
    "            cv2.imshow(\"Detecci√≥n de INE\", frame)\n",
    "\n",
    "            if stable_counter >= required_frames:\n",
    "                cv2.imwrite(\"ine.jpg\", raw_frame)\n",
    "                print(\"üì∏ INE capturada como 'ine.jpg'\")\n",
    "                ine_capturada = True\n",
    "                break\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == 27:\n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                return False\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        if not ine_capturada:\n",
    "            continue\n",
    "\n",
    "        print(\"üîç Procesando INE con OCR...\")\n",
    "        results = ocr.ocr(\"ine.jpg\", cls=True)\n",
    "        lineas_ocr_ine = [line[1][0].strip().upper() for line in results[0]]\n",
    "\n",
    "        curp_match = next((re.search(r\"\\b[A-Z]{4}\\d{6}[A-Z0-9]{8}\\b\", l)\n",
    "                           for l in lineas_ocr_ine if re.search(r\"\\b[A-Z]{4}\\d{6}[A-Z0-9]{8}\\b\", l)), None)\n",
    "        curp = curp_match.group() if curp_match else \"No detectado\"\n",
    "\n",
    "        fecha_match = next((re.search(r\"\\b\\d{2}[/-]\\d{2}[/-]\\d{4}\\b\", l)\n",
    "                            for l in lineas_ocr_ine if re.search(r\"\\b\\d{2}[/-]\\d{2}[/-]\\d{4}\\b\", l)), None)\n",
    "        fecha = fecha_match.group() if fecha_match else \"No detectada\"\n",
    "\n",
    "        print(\"üìå CURP detectado:\", curp)\n",
    "        print(\"üìå Fecha de nacimiento:\", fecha)\n",
    "\n",
    "        print(\"üåê Consultando datos oficiales del CURP...\")\n",
    "        try:\n",
    "            driver = webdriver.Chrome(options=chrome_options)\n",
    "            driver.get(\"https://www.gob.mx/curp/\")\n",
    "\n",
    "            input_curp = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.ID, \"curpinput\"))\n",
    "            )\n",
    "            input_curp.send_keys(curp)\n",
    "            boton_buscar = driver.find_element(By.ID, \"searchButton\")\n",
    "            boton_buscar.click()\n",
    "\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//td[@style='text-transform: uppercase;']\"))\n",
    "            )\n",
    "\n",
    "            celdas = driver.find_elements(By.XPATH, \"//td[@style='text-transform: uppercase;']\")\n",
    "            if len(celdas) >= 7:\n",
    "                nombre_web = celdas[1].text.strip()\n",
    "                apellido1_web = celdas[2].text.strip()\n",
    "                apellido2_web = celdas[3].text.strip()\n",
    "                fecha_web = celdas[5].text.strip()\n",
    "\n",
    "                print(\"üìÑ Datos oficiales:\", apellido1_web, apellido2_web, nombre_web, fecha_web)\n",
    "                nombre_ok = coincide_nombre_web_con_ocr(nombre_web, apellido1_web, apellido2_web, lineas_ocr_ine)\n",
    "                fecha_ok = (fecha.strip() == fecha_web.strip())\n",
    "\n",
    "                if nombre_ok and fecha_ok:\n",
    "                    print(\"‚úÖ Identidad confirmada con datos oficiales.\")\n",
    "                    driver.quit()\n",
    "                    return True\n",
    "                else:\n",
    "                    print(\"‚ùå Los datos de la INE no coinciden con el CURP.\")\n",
    "            else:\n",
    "                print(\"‚ùå No se extrajeron suficientes datos del sitio.\")\n",
    "            driver.quit()\n",
    "            time.sleep(2)\n",
    "\n",
    "        except (WebDriverException, TimeoutException) as e:\n",
    "            print(f\"‚ö†Ô∏è Error en Selenium: {e}\")\n",
    "            print(\"üîÅ Ocurri√≥ un error al consultar el CURP. Volveremos a escanear la INE...\")\n",
    "            try:\n",
    "                driver.quit()\n",
    "            except:\n",
    "                pass\n",
    "            time.sleep(2)\n",
    "            continue  # vuelve al loop a escanear de nuevo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb92630e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparar_rostros_ine_selfie():\n",
    "    import mediapipe as mp\n",
    "    import face_recognition\n",
    "    import cv2\n",
    "    import os\n",
    "\n",
    "    print(\"üß† Detectando rostro en la INE...\")\n",
    "\n",
    "    mp_face_detection = mp.solutions.face_detection\n",
    "    face_detection = mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.6)\n",
    "\n",
    "    image = cv2.imread(\"ine.jpg\")\n",
    "    if image is None:\n",
    "        print(\"‚ùå No se encontr√≥ 'ine.jpg'.\")\n",
    "        return False\n",
    "\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = face_detection.process(image_rgb)\n",
    "\n",
    "    if results.detections:\n",
    "        h, w, _ = image.shape\n",
    "        max_area = 0\n",
    "        best_face = None\n",
    "\n",
    "        for detection in results.detections:\n",
    "            bbox = detection.location_data.relative_bounding_box\n",
    "            x, y = int(bbox.xmin * w), int(bbox.ymin * h)\n",
    "            w_box, h_box = int(bbox.width * w), int(bbox.height * h)\n",
    "            area = w_box * h_box\n",
    "\n",
    "            if area > max_area:\n",
    "                max_area = area\n",
    "                best_face = (x, y, w_box, h_box)\n",
    "\n",
    "        if best_face:\n",
    "            x, y, w_box, h_box = best_face\n",
    "            x, y = max(0, x), max(0, y)\n",
    "            cropped_face = image[y:y + h_box, x:x + w_box]\n",
    "            cv2.imwrite(\"ine_face.jpg\", cropped_face)\n",
    "            print(\"‚úÖ Rostro recortado y guardado como 'ine_face.jpg'\")\n",
    "    else:\n",
    "        print(\"‚ùå No se detect√≥ rostro en la INE.\")\n",
    "        return False\n",
    "\n",
    "    # Comparaci√≥n facial\n",
    "    print(\"üß™ Comparando rostro de INE con selfie...\")\n",
    "\n",
    "    if not os.path.exists(\"selfie.jpg\") or not os.path.exists(\"ine_face.jpg\"):\n",
    "        print(\"‚ùå Faltan im√°genes para la comparaci√≥n facial.\")\n",
    "        return False\n",
    "\n",
    "    img1 = face_recognition.load_image_file(\"selfie.jpg\")\n",
    "    img2 = face_recognition.load_image_file(\"ine_face.jpg\")\n",
    "\n",
    "    enc1 = face_recognition.face_encodings(img1)\n",
    "    enc2 = face_recognition.face_encodings(img2)\n",
    "\n",
    "    if enc1 and enc2:\n",
    "        result = face_recognition.compare_faces([enc1[0]], enc2[0])\n",
    "        distance = face_recognition.face_distance([enc1[0]], enc2[0])[0]\n",
    "\n",
    "        if result[0]:\n",
    "            print(f\"‚úÖ Rostros coinciden (distancia: {distance:.4f})\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå Rostros NO coinciden (distancia: {distance:.4f})\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"‚ùå No se pudieron codificar ambos rostros correctamente.\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c411c8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predecir_deepfake(video_path, model_path=\"mediapipe_model.pth\"):\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    from torchvision import transforms, models\n",
    "    import mediapipe as mp\n",
    "    import os\n",
    "\n",
    "    class DeepfakeDetector(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.cnn = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "            self.cnn.classifier = nn.Identity()\n",
    "            self.embedding_dim = 1280\n",
    "            self.sequence_length = 16\n",
    "\n",
    "            self.lstm = nn.LSTM(input_size=1285, hidden_size=128, num_layers=1,\n",
    "                                batch_first=True, bidirectional=True)\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(256, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(64, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "        def forward(self, x_imgs, x_lmks):\n",
    "            B, T, C, H, W = x_imgs.shape\n",
    "            x_imgs = x_imgs.view(B * T, C, H, W)\n",
    "            features = self.cnn(x_imgs)\n",
    "            features = features.view(B, T, -1)\n",
    "            combined = torch.cat([features, x_lmks], dim=2)\n",
    "            out, _ = self.lstm(combined)\n",
    "            out = out[:, -1, :]\n",
    "            return self.fc(out).squeeze(1)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    sequence_length = 16\n",
    "    candidate_frames = 25\n",
    "    image_size = (256, 256)\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    mp_face_mesh = mp.solutions.face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1,\n",
    "                                                   refine_landmarks=True, min_detection_confidence=0.5)\n",
    "\n",
    "    def extract_landmark_vector(landmarks, frame_shape):\n",
    "        h, w, _ = frame_shape\n",
    "        def norm(x): return x / w\n",
    "        def norm_y(y): return y / h\n",
    "        left_eye = landmarks.landmark[33]\n",
    "        right_eye = landmarks.landmark[263]\n",
    "        nose = landmarks.landmark[1]\n",
    "        mouth_left = landmarks.landmark[61]\n",
    "        mouth_right = landmarks.landmark[291]\n",
    "        return np.array([\n",
    "            norm(left_eye.x), norm(right_eye.x),\n",
    "            norm_y(nose.y),\n",
    "            norm_y(mouth_left.y),\n",
    "            norm_y(mouth_right.y)\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def crop_face_from_landmarks(landmarks, frame):\n",
    "        h, w, _ = frame.shape\n",
    "        x_coords = [lm.x for lm in landmarks.landmark]\n",
    "        y_coords = [lm.y for lm in landmarks.landmark]\n",
    "        min_x, max_x = int(min(x_coords) * w), int(max(x_coords) * w)\n",
    "        min_y, max_y = int(min(y_coords) * h), int(max(y_coords) * h)\n",
    "        margin_x = int((max_x - min_x) * 0.2)\n",
    "        margin_y = int((max_y - min_y) * 0.2)\n",
    "        x1 = max(min_x - margin_x, 0)\n",
    "        y1 = max(min_y - margin_y, 0)\n",
    "        x2 = min(max_x + margin_x, w)\n",
    "        y2 = min(max_y + margin_y, h)\n",
    "        face_crop = frame[y1:y2, x1:x2]\n",
    "        return cv2.resize(face_crop, image_size)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_indices = np.linspace(0, total_frames - 1, candidate_frames).astype(int)\n",
    "\n",
    "    images, landmarks_list = [], []\n",
    "    evidencia_guardada = False\n",
    "\n",
    "    for idx in frame_indices:\n",
    "        if len(images) >= sequence_length:\n",
    "            break\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = mp_face_mesh.process(rgb)\n",
    "        if results.multi_face_landmarks:\n",
    "            try:\n",
    "                lmks = results.multi_face_landmarks[0]\n",
    "                cropped = crop_face_from_landmarks(lmks, frame)\n",
    "                lmk_vector = extract_landmark_vector(lmks, frame.shape)\n",
    "\n",
    "                if not evidencia_guardada:\n",
    "                    cv2.imwrite(\"evidencia.jpg\", cropped)\n",
    "                    evidencia_guardada = True\n",
    "\n",
    "                images.append(transform(cropped))\n",
    "                landmarks_list.append(torch.tensor(lmk_vector, dtype=torch.float32))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    cap.release()\n",
    "    mp_face_mesh.close()\n",
    "\n",
    "    if len(images) < sequence_length:\n",
    "        print(f\"‚ö†Ô∏è Solo se obtuvieron {len(images)} frames v√°lidos. No se puede hacer inferencia.\")\n",
    "        return None\n",
    "\n",
    "    x_imgs = torch.stack(images[:sequence_length]).unsqueeze(0).to(device)\n",
    "    x_lmks = torch.stack(landmarks_list[:sequence_length]).unsqueeze(0).to(device)\n",
    "\n",
    "    model = DeepfakeDetector().to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(x_imgs, x_lmks)\n",
    "        prob = output.item()\n",
    "        label = \"FAKE\" if prob > 0.5 else \"REAL\"\n",
    "        print(f\"\\nüß™ Resultado: {label}  |  Probabilidad: {prob:.4f} | Evidencia: evidencia.jpg\")\n",
    "        return {\"label\": label, \"prob\": prob, \"evidencia\": \"evidencia.jpg\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ca8cae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü° Iniciando reto de vida...\n",
      "‚úÖ Parpadeo #1\n",
      "‚úÖ Parpadeo #2\n",
      "‚úÖ Negaci√≥n #1\n",
      "‚úÖ Parpadeo #3\n",
      "‚úÖ Parpadeos completados\n",
      "‚è±Ô∏è Asentir: tiempo excedido\n",
      "‚è±Ô∏è Negar: tiempo excedido\n",
      "‚úÖ Asentimiento #1\n",
      "‚úÖ Asentimiento #2\n",
      "‚úÖ Asentir completado\n",
      "‚úÖ Negaci√≥n #1\n",
      "‚è±Ô∏è Negar: tiempo excedido\n",
      "‚úÖ Negaci√≥n #1\n",
      "‚úÖ Negaci√≥n #2\n",
      "‚úÖ Negar completado\n",
      "üì∏ Reto de vida completado. Prepara tu selfie...\n",
      "üì∑ Selfie capturada como 'selfie.jpg'\n",
      "üé¨ Reto de vida completado correctamente.\n",
      "[2025/05/07 22:43:15] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, use_gcu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='C:\\\\Users\\\\Hermanos/.paddleocr/whl\\\\det\\\\en\\\\en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='C:\\\\Users\\\\Hermanos/.paddleocr/whl\\\\rec\\\\latin\\\\latin_PP-OCRv3_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='c:\\\\Users\\\\Hermanos\\\\Desktop\\\\Proyecto Deepfake\\\\.venv-mediapipe\\\\lib\\\\site-packages\\\\paddleocr\\\\ppocr\\\\utils\\\\dict\\\\latin_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='C:\\\\Users\\\\Hermanos/.paddleocr/whl\\\\cls\\\\ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, onnx_providers=False, onnx_sess_options=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, formula_algorithm='LaTeXOCR', formula_model_dir=None, formula_char_dict_path=None, formula_batch_num=1, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, formula=False, ocr=True, recovery=False, recovery_to_markdown=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='es', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n",
      "ü™™ Muestra tu INE al centro de la c√°mara, bien enfocada y estable...\n",
      "üì∏ INE capturada como 'ine.jpg'\n",
      "üîç Procesando INE con OCR...\n",
      "[2025/05/07 22:44:15] ppocr DEBUG: dt_boxes num : 25, elapsed : 0.04928088188171387\n",
      "[2025/05/07 22:44:15] ppocr DEBUG: cls num  : 25, elapsed : 0.07489395141601562\n",
      "[2025/05/07 22:44:16] ppocr DEBUG: rec_res num  : 25, elapsed : 0.33364295959472656\n",
      "üìå CURP detectado: FOMJ030313HNLLNSA2\n",
      "üìå Fecha de nacimiento: 13/03/2003\n",
      "üåê Consultando datos oficiales del CURP...\n",
      "üìÑ Datos oficiales: FLORES MENDOZA JOSUE EMMANUEL 13/03/2003\n",
      "‚úÖ Identidad confirmada con datos oficiales.\n",
      "üß† Detectando rostro en la INE...\n",
      "‚úÖ Rostro recortado y guardado como 'ine_face.jpg'\n",
      "üß™ Comparando rostro de INE con selfie...\n",
      "‚úÖ Rostros coinciden (distancia: 0.4384)\n",
      "\n",
      "üß™ Resultado: REAL  |  Probabilidad: 0.3616 | Evidencia: evidencia.jpg\n",
      "Veredicto final: Es una persona REAL\n"
     ]
    }
   ],
   "source": [
    "if realizar_reto_de_vida() == True:\n",
    "    if verificar_ine_con_curp() == True:\n",
    "        if comparar_rostros_ine_selfie() == True:\n",
    "            resultado = predecir_deepfake(\"verificacion_video.mp4\", \"mediapipe_model.pth\")\n",
    "            if resultado:\n",
    "                print(\"Veredicto final: Es una persona\", resultado[\"label\"])\n",
    "            else:\n",
    "                print(\"‚ùå No se pudo analizar el video.\")\n",
    "        else:\n",
    "            print(\"No se pudo comprobar que se tratara de una persona real\")\n",
    "    else:\n",
    "        print(\"No se pudo comprobar que se tratara de una persona real\")\n",
    "else:\n",
    "    print(\"No se pudo comprobar que se tratara de una persona real\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-mediapipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
