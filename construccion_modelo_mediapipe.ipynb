{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d157e54",
   "metadata": {},
   "source": [
    "# Construcción del modelo en mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2720749b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from tensorflow) (4.13.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from tensorflow) (2.2.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from tensorflow) (4.25.7)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from tensorflow) (3.9.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from tensorflow) (57.4.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: optree in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
      "Requirement already satisfied: namex in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Hermanos\\Desktop\\Proyecto Deepfake\\.venv-mediapipe\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9488ef58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from sklearn) (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from scikit-learn->sklearn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from scikit-learn->sklearn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from scikit-learn->sklearn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hermanos\\desktop\\proyecto deepfake\\.venv-mediapipe\\lib\\site-packages (from scikit-learn->sklearn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Hermanos\\Desktop\\Proyecto Deepfake\\.venv-mediapipe\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d41c3545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿CUDA disponible? True\n",
      "GPU actual: NVIDIA GeForce RTX 4070 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"¿CUDA disponible?\", torch.cuda.is_available())\n",
    "print(\"GPU actual:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "142a1e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.utils import shuffle\n",
    "from collections import Counter\n",
    "\n",
    "class DeepfakeDataset(Dataset):\n",
    "    def __init__(self, base_path, sequence_length=16, split='train', val_split=0.2):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.base_path = base_path\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        \n",
    "        # Paso 1: Cargar videos completos con etiquetas\n",
    "        all_videos = self._collect_video_ids(base_path)\n",
    "        all_videos = shuffle(all_videos, random_state=42)  # Mezcla antes de dividir\n",
    "        \n",
    "        # Paso 2: Dividir por video, no por muestra\n",
    "        split_idx = int(len(all_videos) * (1 - val_split))\n",
    "        self.videos = all_videos[:split_idx] if split == 'train' else all_videos[split_idx:]\n",
    "\n",
    "    def _collect_video_ids(self, base_path):\n",
    "        video_samples = []\n",
    "        for folder in os.listdir(base_path):\n",
    "            folder_path = os.path.join(base_path, folder)\n",
    "            label = 0 if folder == \"original\" else 1\n",
    "            video_ids = {}\n",
    "            for file in os.listdir(folder_path):\n",
    "                if file.endswith(\".jpg\"):\n",
    "                    vid = \"_\".join(file.split(\"_\")[:-1])\n",
    "                    video_ids.setdefault(vid, []).append(file)\n",
    "            for vid, files in video_ids.items():\n",
    "                if len(files) >= self.sequence_length:\n",
    "                    video_samples.append((folder_path, vid, label))\n",
    "        return video_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.videos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        folder, video_id, label = self.videos[idx]\n",
    "        imgs = []\n",
    "        lmks = []\n",
    "        for i in range(self.sequence_length):\n",
    "            img_path = os.path.join(folder, f\"{video_id}_{i}.jpg\")\n",
    "            lmk_path = os.path.join(folder, f\"{video_id}_{i}.npy\")\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.resize(img, (256, 256))\n",
    "            img = self.transform(img)  # (3, 256, 256)\n",
    "            lmk = torch.tensor(np.load(lmk_path), dtype=torch.float32)\n",
    "            imgs.append(img)\n",
    "            lmks.append(lmk)\n",
    "        imgs = torch.stack(imgs)      # (16, 3, 256, 256)\n",
    "        lmks = torch.stack(lmks)      # (16, 5)\n",
    "        return imgs, lmks, torch.tensor(label, dtype=torch.float32)\n",
    "    \n",
    "\n",
    "# ✅ Diagnóstico: Conteo de clases reales/falsas\n",
    "def print_class_distribution(dataset, name):\n",
    "    labels = [int(label.item()) for _, _, label in dataset]\n",
    "    counts = Counter(labels)\n",
    "    print(f\"{name} class distribution:\")\n",
    "    print(f\"  Real (0): {counts.get(0, 0)}\")\n",
    "    print(f\"  Fake (1): {counts.get(1, 0)}\")\n",
    "    print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9bd4f7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "class DeepfakeDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "        self.cnn.classifier = nn.Identity()  # elimina la capa final\n",
    "        self.embedding_dim = 1280\n",
    "        self.sequence_length = 16\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=1285, hidden_size=128, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x_imgs, x_lmks):\n",
    "        B, T, C, H, W = x_imgs.shape\n",
    "        x_imgs = x_imgs.view(B * T, C, H, W)\n",
    "        features = self.cnn(x_imgs)                     # (B*T, 1280)\n",
    "        features = features.view(B, T, -1)              # (B, T, 1280)\n",
    "        combined = torch.cat([features, x_lmks], dim=2) # (B, T, 1285)\n",
    "        out, _ = self.lstm(combined)\n",
    "        out = out[:, -1, :]                             # última salida\n",
    "        return self.fc(out).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45924490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class distribution:\n",
      "  Real (0): 768\n",
      "  Fake (1): 723\n",
      "------------------------------\n",
      "Validation class distribution:\n",
      "  Real (0): 207\n",
      "  Fake (1): 166\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "base_path = r\"C:/Users/Hermanos/Desktop/Proyecto Deepfake/preprocesamiento_mediapipe\"\n",
    "\n",
    "train_dataset = DeepfakeDataset(base_path, split='train')\n",
    "val_dataset = DeepfakeDataset(base_path, split='val')\n",
    "\n",
    "print_class_distribution(train_dataset, \"Train\")\n",
    "print_class_distribution(val_dataset, \"Validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61e2577f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=0.6595 | Val Accuracy=0.7507 | Val F1-score=0.7424\n",
      "✅ Modelo guardado con F1-score = 0.7424\n",
      "Epoch 2: Loss=0.5283 | Val Accuracy=0.8874 | Val F1-score=0.8750\n",
      "✅ Modelo guardado con F1-score = 0.8750\n",
      "Epoch 3: Loss=0.3916 | Val Accuracy=0.8740 | Val F1-score=0.8698\n",
      "Epoch 4: Loss=0.3354 | Val Accuracy=0.9008 | Val F1-score=0.8969\n",
      "✅ Modelo guardado con F1-score = 0.8969\n",
      "Epoch 5: Loss=0.2427 | Val Accuracy=0.9303 | Val F1-score=0.9217\n",
      "✅ Modelo guardado con F1-score = 0.9217\n",
      "Epoch 6: Loss=0.1966 | Val Accuracy=0.7909 | Val F1-score=0.8079\n",
      "Epoch 7: Loss=0.1488 | Val Accuracy=0.9491 | Val F1-score=0.9408\n",
      "✅ Modelo guardado con F1-score = 0.9408\n",
      "Epoch 8: Loss=0.1300 | Val Accuracy=0.9357 | Val F1-score=0.9245\n",
      "Epoch 9: Loss=0.0682 | Val Accuracy=0.9196 | Val F1-score=0.9143\n",
      "Epoch 10: Loss=0.0860 | Val Accuracy=0.9357 | Val F1-score=0.9264\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset y loaders\n",
    "train_dataset = DeepfakeDataset(base_path=r\"C:/Users/Hermanos/Desktop/Proyecto Deepfake/preprocesamiento_mediapipe\", split='train')\n",
    "val_dataset = DeepfakeDataset(base_path=r\"C:/Users/Hermanos/Desktop/Proyecto Deepfake/preprocesamiento_mediapipe\", split='val')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Modelo\n",
    "model = DeepfakeDetector().to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "best_f1 = 0.0\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for imgs, lmks, labels in train_loader:\n",
    "        imgs, lmks, labels = imgs.to(device), lmks.to(device), labels.to(device)\n",
    "        preds = model(imgs, lmks)\n",
    "        loss = criterion(preds, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validación\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, lmks, labels in val_loader:\n",
    "            imgs, lmks = imgs.to(device), lmks.to(device)\n",
    "            outputs = model(imgs, lmks)\n",
    "            preds = (outputs > 0.5).cpu().numpy()\n",
    "            y_pred.extend(preds)\n",
    "            y_true.extend(labels.numpy())\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"Epoch {epoch}: Loss={avg_loss:.4f} | Val Accuracy={acc:.4f} | Val F1-score={f1:.4f}\")\n",
    "\n",
    "    # Guardar mejor modelo\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        torch.save(model.state_dict(), \"mediapipe_model.pth\")\n",
    "        print(f\"✅ Modelo guardado con F1-score = {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b8c9f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preds: Counter({0: 213, 1: 160})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import Counter\n",
    "print(\"Preds:\", Counter(np.array(y_pred, dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7fdb5239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        real       0.93      0.96      0.94       207\n",
      "        fake       0.94      0.91      0.93       166\n",
      "\n",
      "    accuracy                           0.94       373\n",
      "   macro avg       0.94      0.93      0.93       373\n",
      "weighted avg       0.94      0.94      0.94       373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred, target_names=[\"real\", \"fake\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624b4cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-mediapipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
